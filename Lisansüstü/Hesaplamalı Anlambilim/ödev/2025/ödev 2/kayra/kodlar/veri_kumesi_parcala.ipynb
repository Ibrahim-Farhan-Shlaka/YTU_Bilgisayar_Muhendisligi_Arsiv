{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giriş işlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas numpy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kök dizin belirleme\n",
    "if is_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    kok_dizin = \"/content/drive/MyDrive/TurkNLP-ModelBench\"\n",
    "else:\n",
    "    kok_dizin = os.getcwd()\n",
    "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(kok_dizin)\n",
    "from dizin_yardimcisi import veri_kumesi_dizin_al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri kümesi ön işlemleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Kümesi İçin Kök Klasör Oluştur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri kümesi dosyalarının kaydedileceği dizin\n",
    "veri_kumeleri_dizini = veri_kumesi_dizin_al()\n",
    "os.makedirs(veri_kumeleri_dizini, exist_ok=True)\n",
    "print(f\"Veri kümeleri dizini: {veri_kumeleri_dizini}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri kümeleri ve parçalama fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veri_kumesi_yukle(koleksiyon_adi, veri_kumesi_adi):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    koleksiyon_adi (str): Veri kümesinin bulunduğu koleksiyon adı\n",
    "    veri_kumesi_adi (str): Yüklenecek veri kümesinin adı\n",
    "\n",
    "    Returns:\n",
    "    veri_kumesi (datasets.Dataset): Yüklenen veri kümesi\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Veri kümesi yükleniyor: {veri_kumesi_adi}\")\n",
    "    veri_kumesi = load_dataset(koleksiyon_adi, veri_kumesi_adi)\n",
    "    print(f\"Veri kümesi yüklendi: {veri_kumesi_adi}\")\n",
    "    return veri_kumesi\n",
    "def boyut_kontrol(veri_kumesi, veri_kumesi_parca_boyutu) -> bool:\n",
    "    \"\"\"\n",
    "    Veri kümesinin boyutunu kontrol eder.\n",
    "    \n",
    "    Args:\n",
    "    veri_kumesi (datasets.Dataset): Kontrol edilecek veri kümesi\n",
    "    veri_kumesi_parca_boyutu (int): veri kümesinden istenilen parçanın boyutu\n",
    "    \n",
    "    Returns:\n",
    "    bool: Veri kümesi belirtilen boyuttan büyükse True, aksi halde False\n",
    "    \"\"\"\n",
    "    if veri_kumesi_parca_boyutu is not None and len(veri_kumesi) < veri_kumesi_parca_boyutu:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def veri_kumesi_parcala(veri_kumesi, egitim_boyutu=None, sinama_boyutu=None, tohum_degeri=42):\n",
    "    \"\"\"\n",
    "    Veri kümesini belirli büyüklükteki eğitim ve test alt kümelerine ayırır.\n",
    "    \n",
    "    Parametreler:\n",
    "        veri_kumesi: Parçalanacak veri kümesi\n",
    "        egitim_boyutu: Eğitim kümesi büyüklüğü (None ise tamamını kullanır)\n",
    "        sinama_boyutu: Test kümesi büyüklüğü (None ise tamamını kullanır)\n",
    "        tohum_degeri: Rastgele seçim için kullanılacak tohum değeri\n",
    "        \n",
    "    Dönüş:\n",
    "        egitim_kumesi, sinama_kumesi, sayilar_dict: Eğitim ve test veri kümeleri ve etiket sayıları\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.RandomState(tohum_degeri)\n",
    "    \n",
    "    # Eğitim ve test kümelerini al\n",
    "    egitim_kumesi = veri_kumesi[\"train\"]\n",
    "    sinama_kumesi = veri_kumesi[\"test\"]\n",
    "    \n",
    "    sayilar_dict = {'egitim': {}, 'sinama': {}}\n",
    "    \n",
    "    # Eğitim kümesi işleme\n",
    "    egitim_kumesi, egitim_sayilari = alt_kume_olustur(egitim_kumesi, egitim_boyutu, rng)\n",
    "    sayilar_dict['egitim'] = egitim_sayilari\n",
    "    \n",
    "    # Test kümesi işleme\n",
    "    sinama_kumesi, sinama_sayilari = alt_kume_olustur(sinama_kumesi, sinama_boyutu, rng)\n",
    "    sayilar_dict['sinama'] = sinama_sayilari\n",
    "    \n",
    "    return egitim_kumesi, sinama_kumesi, sayilar_dict\n",
    "\n",
    "def alt_kume_olustur(veri_kumesi, boyut, rng):\n",
    "    \"\"\"\n",
    "    Veri kümesinden istenen boyutta dengeli bir alt küme oluşturur.\n",
    "    \n",
    "    Parametreler:\n",
    "        veri_kumesi: İşlenecek veri kümesi\n",
    "        boyut: İstenen toplam örnek sayısı (None ise tamamını kullanır)\n",
    "        rng: Rastgele sayı üretici\n",
    "        \n",
    "    Dönüş:\n",
    "        alt_kume, etiket_sayilari: Alt küme ve etiketlerin sayıları\n",
    "    \"\"\"\n",
    "    # Etiketleri al\n",
    "    etiketler = np.array(veri_kumesi['label'])\n",
    "    \n",
    "    # Mevcut benzersiz etiketleri bul\n",
    "    benzersiz_etiketler = np.unique(etiketler)\n",
    "    \n",
    "    # 0 ve 1 etiketleri için indeksleri al (bu veri kümelerinin sadece 0 ve 1 etiketi var)\n",
    "    indeksler_0 = np.where(etiketler == 0)[0] if 0 in benzersiz_etiketler else np.array([])\n",
    "    indeksler_1 = np.where(etiketler == 1)[0] if 1 in benzersiz_etiketler else np.array([])\n",
    "    diger_indeksler = np.where((etiketler != 0) & (etiketler != 1))[0]\n",
    "    \n",
    "    # Her bir etiketin sayısı\n",
    "    sayi_0 = len(indeksler_0)\n",
    "    sayi_1 = len(indeksler_1)\n",
    "    sayi_diger = len(diger_indeksler)\n",
    "    \n",
    "    # Her bir etiketten alınacak örnek sayısı\n",
    "    if boyut is None:\n",
    "        # İstenen boyut verilmemişse, mevcut verilerin tamamını al\n",
    "        num_0 = sayi_0\n",
    "        num_1 = sayi_1\n",
    "        num_diger = sayi_diger\n",
    "    else:\n",
    "        # İki ana sınıftan (0 ve 1) eşit sayıda örnek almaya çalış\n",
    "        # Toplam boyut ikiye bölünmeli\n",
    "        if sayi_0 > 0 and sayi_1 > 0:\n",
    "            # Her iki etiketten de alınabilecek maksimum eşit sayı\n",
    "            hedef_etiket_sayisi = boyut // 2\n",
    "            \n",
    "            # Her sınıftan en fazla mevcut olan kadar alabiliriz\n",
    "            num_0 = min(sayi_0, hedef_etiket_sayisi)\n",
    "            num_1 = min(sayi_1, hedef_etiket_sayisi)\n",
    "            \n",
    "            # Hedeflenen toplam örneklerden kalan boşluk\n",
    "            kalan_boyut = boyut - (num_0 + num_1)\n",
    "            \n",
    "            # Kalan boşluğu doldur (önce eksik kalan etiketler için)\n",
    "            if kalan_boyut > 0:\n",
    "                if sayi_0 > num_0:  # 0 etiketinden daha fazla alabilir miyiz?\n",
    "                    ek_0 = min(sayi_0 - num_0, kalan_boyut)\n",
    "                    num_0 += ek_0\n",
    "                    kalan_boyut -= ek_0\n",
    "                    \n",
    "                if kalan_boyut > 0 and sayi_1 > num_1:  # 1 etiketinden daha fazla alabilir miyiz?\n",
    "                    ek_1 = min(sayi_1 - num_1, kalan_boyut)\n",
    "                    num_1 += ek_1\n",
    "                    kalan_boyut -= ek_1\n",
    "                    \n",
    "                # Hala kalan boşluk varsa diğer etiketlerden doldur\n",
    "                num_diger = min(kalan_boyut, sayi_diger)\n",
    "            else:\n",
    "                num_diger = 0\n",
    "        else:\n",
    "            # Eğer bir etiket hiç yoksa, diğerinden mümkün olduğu kadar al\n",
    "            if sayi_0 > 0:\n",
    "                num_0 = min(sayi_0, boyut)\n",
    "                num_1 = 0\n",
    "            elif sayi_1 > 0:\n",
    "                num_0 = 0\n",
    "                num_1 = min(sayi_1, boyut)\n",
    "            else:\n",
    "                num_0 = 0\n",
    "                num_1 = 0\n",
    "            \n",
    "            # Kalan boşluğu diğer etiketlerden doldur\n",
    "            num_diger = min(boyut - (num_0 + num_1), sayi_diger)\n",
    "    \n",
    "    # Rastgele indeksleri seç\n",
    "    secilen_indeksler = []\n",
    "    \n",
    "    if num_0 > 0:\n",
    "        secilen_indeksler.extend(rng.choice(indeksler_0, size=num_0, replace=False))\n",
    "    if num_1 > 0:\n",
    "        secilen_indeksler.extend(rng.choice(indeksler_1, size=num_1, replace=False))\n",
    "    if num_diger > 0:\n",
    "        secilen_indeksler.extend(rng.choice(diger_indeksler, size=num_diger, replace=False))\n",
    "    \n",
    "    # İndeksleri karıştır \n",
    "    rng.shuffle(secilen_indeksler)\n",
    "    \n",
    "    # Alt kümeyi oluştur\n",
    "    alt_kume = veri_kumesi.select(secilen_indeksler)\n",
    "    \n",
    "    # Etiketlerin sayısını hesapla\n",
    "    alt_etiketler = np.array(alt_kume['label'])\n",
    "    unique, counts = np.unique(alt_etiketler, return_counts=True)\n",
    "    \n",
    "    # Etiket sayılarını düz int'e dönüştür\n",
    "    etiket_sayilari = {str(int(u)) + ' sayisi': int(c) for u, c in zip(unique, counts)}\n",
    "    \n",
    "    return alt_kume, etiket_sayilari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri kümeleri indirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"turkish-nlp-suite/TrGLUE\"\n",
    "sst2_name = \"sst2\"\n",
    "cola_name = \"cola\"\n",
    "tohum_degeri = 571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = veri_kumesi_yukle(dataset_name, sst2_name)\n",
    "cola_dataset = veri_kumesi_yukle(dataset_name, cola_name)\n",
    "print(f\"Veri kümeleri yüklendi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri kümelerini ayrıştırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sst2 veri kümesi parçalanıyor...\")\n",
    "sst2_egitim, sst2_sinama, sst2_dict = veri_kumesi_parcala(sst2_dataset, egitim_boyutu=5000, sinama_boyutu=1000, tohum_degeri=tohum_degeri)\n",
    "print(\"cola veri kümesi parçalanıyor...\")\n",
    "cola_egitim, cola_sinama, cola_dict = veri_kumesi_parcala(cola_dataset, egitim_boyutu=5000, tohum_degeri=tohum_degeri)\n",
    "print(\"Veri kümeleri parçalandı.\")\n",
    "print(f\"Veri kümesi boyutları:\\nSST-2: Eğitim: {len(sst2_egitim)}, Sinama: {len(sst2_sinama)}\\nCoLA: Eğitim: {len(cola_egitim)}, Sinama: {len(cola_sinama)}\")\n",
    "print(f\"\\nsst2 eğitim kümesi:\\n{sst2_egitim[0:5]}\\n\\nsst2 sinama kümesi:\\n{sst2_sinama[0:5]}\\n\\ncola eğitim kümesi:\\n{cola_egitim[0:5]}\\n\\ncola sinama kümesi:\\n{cola_sinama[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayrıştırılan Veri Kümelerini Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veri_kumesi_kaydet(veri_kumesi, dosya_adi):\n",
    "    \"\"\"\n",
    "    Veri kümesini csv dosyası olarak kaydeder.\n",
    "    \n",
    "    Args:\n",
    "    veri_kumesi (datasets.Dataset): Kaydedilecek veri kümesi\n",
    "    dosya_adi (str): Kaydedilecek dosyanın adı\n",
    "    \"\"\"\n",
    "    print(f\"Veri kümesi kaydediliyor: {dosya_adi}\")\n",
    "    veri_kumesi_df = pd.DataFrame(veri_kumesi)\n",
    "    dosya_yolu = os.path.join(veri_kumeleri_dizini, dosya_adi)\n",
    "    veri_kumesi_df.to_csv(dosya_yolu, index=False)\n",
    "    print(f\"Veri kümesi kaydedildi: {dosya_yolu}\")\n",
    "import json\n",
    "def veri_kumesi_dict_kaydet(veri_kumesi_dict, dosya_adi):\n",
    "    \"\"\"\n",
    "    Veri kümesi bilgilerini csv dosyası olarak kaydeder.\n",
    "    \n",
    "    Args:\n",
    "    veri_kumesi_dict (dict): Kaydedilecek veri kümesi bilgileri\n",
    "    dosya_adi (str): Kaydedilecek dosyanın adı\n",
    "    \"\"\"\n",
    "    print(f\"Veri kümesi dict kaydediliyor: {dosya_adi}\")\n",
    "    veri_kumesi_dict_json = json.dumps(veri_kumesi_dict, indent=4)\n",
    "    dosya_yolu = os.path.join(veri_kumeleri_dizini, dosya_adi)\n",
    "    with open(dosya_yolu, 'w') as f:\n",
    "        f.write(veri_kumesi_dict_json)\n",
    "    print(f\"Veri kümesi dict kaydedildi: {dosya_yolu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_kumesi_kaydet(sst2_egitim, \"sst2_egitim.csv\")\n",
    "veri_kumesi_kaydet(sst2_sinama, \"sst2_sinama.csv\")\n",
    "veri_kumesi_kaydet(cola_egitim, \"cola_egitim.csv\")\n",
    "veri_kumesi_kaydet(cola_sinama, \"cola_sinama.csv\")\n",
    "veri_kumesi_dict_kaydet(sst2_dict, \"sst2_dict.json\")\n",
    "veri_kumesi_dict_kaydet(cola_dict, \"cola_dict.json\")\n",
    "print(\"TÜm veri kümeleri kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
